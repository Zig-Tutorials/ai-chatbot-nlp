{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8e28e0",
   "metadata": {},
   "source": [
    "# Chatbot Model Training Notebook\n",
    "\n",
    "This notebook demonstrates how to preprocess text data, train an LSTM-based model for intent classification, and save the model for later use in a FastAPI chatbot application.\n",
    "\n",
    "The training data is loaded from `../data/intents.json` and includes a few sample intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation and model building\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For saving the tokenizer and label encoder\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the intents JSON file which contains training data\n",
    "# Since the notebook is inside the 'notebooks/' folder, we use '../data/intents.json' to navigate to the data folder\n",
    "with open('../data/intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Display the loaded data (optional)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80abcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "\n",
    "# Initialize lists to store input sentences and corresponding labels\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "# Loop over each intent in the JSON data\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        sentences.append(pattern)  # Add each pattern (user input) to the sentences list\n",
    "        labels.append(intent['tag'])  # Add the corresponding tag to the labels list\n",
    "\n",
    "# Output the first few examples to verify\n",
    "print('Sentences:', sentences[:5])\n",
    "print('Labels:', labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce31ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data to convert words into numerical sequences\n",
    "\n",
    "# Initialize the Tokenizer with an out-of-vocabulary token\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)  # Build the vocabulary based on the input sentences\n",
    "word_index = tokenizer.word_index  # A dictionary mapping words to their index\n",
    "\n",
    "# Convert sentences into sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Determine the maximum sequence length to pad all sequences uniformly\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences so that each sequence has the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Display an example sequence\n",
    "print('Example sequence:', padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6762e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels (tags) into numeric format\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# Transform the text labels into integers\n",
    "encoded_labels = label_encoder.transform(labels)\n",
    "\n",
    "# Convert the integer labels into one-hot encoded vectors\n",
    "num_classes = len(label_encoder.classes_)\n",
    "one_hot_labels = tf.keras.utils.to_categorical(encoded_labels, num_classes=num_classes)\n",
    "\n",
    "# Display the first few encoded labels\n",
    "print('Encoded labels:', encoded_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the LSTM model for intent classification\n",
    "\n",
    "# Build a Sequential model with an Embedding layer, LSTM layers, and Dense layers\n",
    "model = Sequential([\n",
    "    # The Embedding layer converts each word (integer) into a dense vector of fixed size\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_length),\n",
    "    \n",
    "    # The first LSTM layer processes the sequence data\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),  # Dropout helps prevent overfitting\n",
    "    \n",
    "    # A second LSTM layer to further capture sequential patterns\n",
    "    LSTM(32),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # A Dense layer to learn intermediate features\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and the Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the padded sequences and one-hot encoded labels\n",
    "\n",
    "# Here, we train for 30 epochs with a batch size of 8.\n",
    "# Adjust the number of epochs and batch size as needed for your dataset.\n",
    "history = model.fit(padded_sequences, one_hot_labels, epochs=30, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the '../model/' directory\n",
    "model.save('../model/chatbot_model.h5')\n",
    "\n",
    "# Also, save the tokenizer and label encoder for later use during inference\n",
    "with open('../model/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../model/label_encoder.pickle', 'wb') as enc_file:\n",
    "    pickle.dump(label_encoder, enc_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Model and preprocessing objects saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96988a",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "After training the model, you can move on to building the FastAPI app (`app/main.py`) which will load the model, tokenizer, and label encoder to serve the chatbot in real time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}